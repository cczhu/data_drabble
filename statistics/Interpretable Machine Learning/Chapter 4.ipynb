{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning Chapter 4\n",
    "\n",
    "These are notes taken on [Chapter 4 of Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/simple.html).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Some models are inherently interpretable. These models are:\n",
    "\n",
    "- **Linear** if there's a linear relationship between features and target.\n",
    "- **Monotonic** if an increase in a feature always increases the target.\n",
    "\n",
    "Some models can also automatically generate interactions (eg. decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "A typical linear regression model looks like:\n",
    "\n",
    "$$\n",
    "y_i = {\\bf \\beta} \\cdot {\\bf x}_i + \\epsilon\n",
    "$$\n",
    "\n",
    "where $x_{ij}$ is feature $j$ of observation $i$, $x_{i0} = 1$ for the intercept, and $\\beta$ is the vector of coefficients. $\\epsilon$ is the Gaussian error term. Ordinary least squares is the standard way of obtaining the optimal ${\\bf \\hat{\\beta}}$.\n",
    "\n",
    "Assumptions:\n",
    "- Linearity (model form)\n",
    "- Normality (due to $\\epsilon$)\n",
    "- Homoscedasticity ($\\epsilon$ is independent of observation)\n",
    "- Independence between observations (no interaction terms between $i$)\n",
    "- Fixed features ($\\beta$ not allowed to vary between $i$)\n",
    "- Absence of multicollinearity (for unregularized models)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "**Numerical feature**: increasing the numerical feature by one unit changes the estimated outcome by $\\beta_{j}$.\n",
    "\n",
    "**Binary feature**: as above, but can only take on two values - 0 (the \"reference\"), or 1. The effect of moving away from the reference is given by $\\beta_{j}$.\n",
    "\n",
    "**Unordered multicategorical feature**: typically gets turned into $n - 1$ binary features for $n$ possible states.\n",
    "\n",
    "**Intercept $\\beta_0$**: value the target takes when all features have zero value (for binary features, 0 should be their default, or reference, state). All features being zero may not be a possible state. A way for them to make sense is to standardize the features (mean subtraction and normalization).\n",
    "\n",
    "**Adjusted $R^2$**: $R^2$ is given by:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i(y_i - \\hat{y}_i)^2}{\\sum_i(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Adjustable $R^2$ is given by\n",
    "\n",
    "$$\n",
    "\\bar{R}^2 = 1 - (1 - R^2)\\frac{n - 1}{n - p - 1}\n",
    "$$\n",
    "\n",
    "where $p$ is the number of features and $n$ the number of observations.\n",
    "\n",
    "**Feature importance**: the t-statistic of $\\hat{\\beta}_j$ is given by\n",
    "\n",
    "$$\n",
    "t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j}{\\mathrm{SE}(\\hat{\\beta}_j)}\n",
    "$$\n",
    "\n",
    "where the standard error $\\mathrm{SE}(\\hat{\\beta}_j)$ is estimated by using $\\mathrm{SE}(\\hat{\\beta}_j) \\equiv \\sqrt{\\mathrm{Var}(\\hat{\\beta}_j)}$ (by the definition of [standard error](https://en.wikipedia.org/wiki/Standard_error)). This variance can directly be calculated using $\\sqrt{\\mathrm{Var}(\\hat{\\beta}_j)} = \\hat{\\sigma}^2({\\bf X}{\\bf X})^{-1}$, where the mean square error estimate $\\hat{\\sigma}^2 = \\sum_i(y_i - \\hat{y}_i)^2$ (see [here](https://stats.stackexchange.com/a/44841)). This can be used to rank the importance of features. (Based on the definition of the t-statistic, it can also be used to test for how statistically significant it is that $\\hat{\\beta}_j$ is non-zero.)\n",
    "\n",
    "### Visual Interpretation\n",
    "\n",
    "**Weight plot**: Plot $\\beta$ values and confidence intervals on the x-axis, $\\beta$ feature labels on the y. Can make it difficult to see effects if features aren't normalized.\n",
    "\n",
    "**Effect plot**: Plot $\\beta_j \\times x_{ij}$ values. The text recommends plotting the distribution of effects by calculating the $\\beta_j \\times x_{ij}$ quantiles for each $j$ (by calculating them for each $i$ then determining eg. the median of that population). A [box plot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.boxplot.html) is effective to illustrate these quantiles for each feature.\n",
    "\n",
    "### Explaining Individual Predictions\n",
    "\n",
    "We can plot $\\beta_j \\times x_{ij}$ as points on the effect plot. This gives the user a sense of both individual feature contributions and how these contributions compare to the overall distributions for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM, GAM and More\n",
    "\n",
    "### GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('bditto': venv)",
   "language": "python",
   "name": "python36964bitbdittovenvd56842b7ca0c4929925d70f8c4c7f13a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
