{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Logistic Regression\n",
    "\n",
    "## 11.1 - Modelling Conditional Probabilities\n",
    "\n",
    "Suppose we wish to know the conditional probability of a binomial response $Y$ given input variables $X$. Thus, $Y \\in {0, 1}$ and $P(Y) = E[Y]$ (from the property of the Bernoulli RV).\n",
    "\n",
    "Assume $P(Y=1|X=x) = p(x;\\theta)$ for some function $p$ parametrized by $\\theta$. Also assume observations $y_i$ are independent. The conditional likelihood function for the data is then:\n",
    "\n",
    "$$\n",
    "\\prod_i P(Y=y_i|X=x_i) = \\prod_i p(x_i;\\theta)^{y_i}(1 - p(x_i;\\theta))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "where we've used the neat fact that anything with zero exponent is unity to get around having to write separate cases for the positive and negative responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 - Logistic Regression\n",
    "\n",
    "We now need to determine a form for $p(x_i;\\theta)$ that we can use to maximize $\\prod_i P$. A functional form that is bounded between 0 and 1 is:\n",
    "\n",
    "$$\n",
    "\\ln\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + x \\cdot \\beta\n",
    "$$\n",
    "\n",
    "Solving for $p$, we get the famed sigmoidal form of logistic regression:\n",
    "\n",
    "$$\n",
    "p(x;\\beta) = \\frac{1}{1 + \\exp\\left(-(\\beta_0 + x \\cdot \\beta)\\right)}\n",
    "$$\n",
    "\n",
    "(*Of course there's a deeper reason why this form is chosen - I believe it's the natural result of a GLM using a Bernoulli RV dependent variable and canonical link.* This is also the Boltzmann distribution for a two-state system whose energy difference is $\\beta_0 + x \\cdot \\beta$.)\n",
    "\n",
    "When using this as a classifier, we minimize our discretization error if we set our decision boundary to be $p = 0.5$ - everything above this value gets labeled as $1$, and everything below $0$. This occurs when the exponent in the denominator is unity, so the boundary is:\n",
    "\n",
    "$$\n",
    "\\beta_0 + x \\cdot \\beta = 0\n",
    "$$\n",
    "\n",
    "The distance from the decision boundary is\n",
    "\n",
    "$$\n",
    "\\frac{1}{||\\beta||}(\\beta_0 + x \\cdot \\beta) = \\frac{1}{\\sqrt{\\beta\\cdot\\beta}}(\\beta_0 + x \\cdot \\beta)\n",
    "$$\n",
    "\n",
    "and given the equation for $p(x;\\beta)$ the probability changes the most orthogonally to the decision boundary (maximizing the rate of change of the distance).\n",
    "\n",
    "Since logistic regression estimates $p(x;\\beta)$, it serves as more than a mere classification algorithm; however, divining more detailed predictions using $p(x;\\beta)$ gives the model more chance to fail.\n",
    "\n",
    "There are four reasons for logistic regression's popularity:\n",
    "1. Tradition.\n",
    "2. $\\ln(p/(1-p))$ is a continuous generalization of a contingency table with two columns and $n$ rows. In the discrete case, where independent variables are binned, $n$ is finite (and is usually small when dealing with eg. drug trials), while in logistic regression $x$ is assumed to be continuous and therefore $n$ is infinite.\n",
    "3. It's closely related to exponential family distributions, where the probability of a vector $v$ is $p(v) \\propto \\exp\\{\\beta_0 + \\sum_j f_j(v)\\beta_j \\}$ - if a component of $v$ is binary, and $f_j$ is the identity function for all $j$, then we recover logistic regression.\n",
    "4. It works well as a classifier, but this shouldn't be taken to mean it correcty estimates $p$ values.\n",
    "\n",
    "For simplicity, we work with the log-likelihood:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "l &=& \\sum_i -\\ln(1 + \\exp{\\beta_0 + x_i \\cdot \\beta}) + \\sum_i y_i(\\beta_0 + x_i \\cdot \\beta) \\\\\n",
    "\\frac{\\partial l}{\\partial \\beta_j} &=& - \\sum_i(y_i - p(x_i; \\beta_0,\\beta))x_{ij}\n",
    "\\end{eqnarray}\n",
    "\n",
    "While this has no analytic solution (since it's transcendental) we can solve it numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 - Generalized Linear and Additive Models\n",
    "\n",
    "*(Skipping 11.3 since it's about Newton-Raphson and I kind of don't care.)*\n",
    "\n",
    "Logistic regression is a case of generalized linear model (GLM) when the response is [Bernoulli, binomial, multinomial or categorical](https://en.wikipedia.org/wiki/Generalized_linear_model).\n",
    "\n",
    "Perfect classification - one caution about using MLE to fit logistic regression is that it works badly when the data are clearly separable. The reason is because the best-fitting $p$ values will be either 0 or 1, which lead to arbitrarily large magnitude values of $\\beta_0 + x \\cdot \\beta)$.\n",
    "\n",
    "We can also further generalize the relationship $\\beta_0 + x \\cdot \\beta)$ to $\\beta_0 + \\vec{f}\\cdot\\vec{x})$, where $\\vec{f}$ is a non-linear operator (with the same dimensions as $\\vec{x}$) for each dimension of $\\vec{x}$. This is known as a [generalized additive model](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/addmodels.pdf) (GAM). GAMs can also be used to check GLMs in the same way that smoothers can be used to check parametric regressions - fit a GAM and a GLM to data, then generate data using the GLM and repeately fit both models to generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 - Model Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('bditto': venv)",
   "language": "python",
   "name": "python36964bitbdittovenvd56842b7ca0c4929925d70f8c4c7f13a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
