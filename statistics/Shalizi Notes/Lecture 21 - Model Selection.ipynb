{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "These notes summarize [Lecture 21](http://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/21/lecture-21.pdf) of Shalizi's Modern Regression course.\n",
    "\n",
    "## Generalization\n",
    "\n",
    "Using **OLS** as our generic model ($y = \\beta x + \\epsilon$), typically we estimate model coefficients by minimizing the mean square error. We might consider doing model selection using the same metric, but this doesn't lead to a well-predicting model.\n",
    "\n",
    "For a model to be well-predictive, we want the model to make low-MSE predictions when faced with *novel, random* data. That is, a dataset $(X, Y)$ where the independent variables $X$ don't have to be identical to the ones the model was trained with.\n",
    "\n",
    "A slightly easier task would be for the model to make low-MSE predictions when faced with *novel* data with the *same* $X$ data that it was trained with. Since the relationship between $X$ and $Y$ involves both signal and noise, this will switch out one noise profile for another drawn from the same distribution. If the model can't predict $Y$ using this data, it means we have overfit - the model has learned to estimate the noise as well as the data.\n",
    "\n",
    "Assume the novel, out-of-sample data is ${\\bf Y}'$, and in-sample ${\\bf Y}$. The independent variables and coefficients combine to make the predictions, ${\\bf x}\\hat{\\beta} = \\hat{m}$. The out-of-sample MSE is\n",
    "\n",
    "$$\n",
    "E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i' - \\hat{m}_i\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "the in-sample MSE is\n",
    "\n",
    "$$\n",
    "E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i - \\hat{m}_i\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Recalling that $E[X^2] = \\mathrm{Var}[X] + E[X]^2$, and also recognizing that $\\hat{m}$ and $Y_i$ are correlated (since $\\hat{\\beta}$ was found by minimizing the MSE using ${\\bf Y}$) but $\\hat{m}$ and $Y_i'$ are not (since we're assuming the error term in $Y$ is IID - see footnote 3 of Shalizi), we get:\n",
    "\n",
    "$$\n",
    "E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i' - \\hat{m}_i\\right)^2\\right] = E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i - \\hat{m}_i\\right)^2\\right] + \\frac{2}{n}\\sum_{i=1}^n\\mathrm{Cov}\\left[Y_i, \\hat{m}_i\\right]\n",
    "$$\n",
    "\n",
    "For linear models, the last term can be simplified using Shalizi's homework exercises (which I refuse to do) to:\n",
    "\n",
    "$$\n",
    "E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i' - \\hat{m}_i\\right)^2\\right] = E\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i - \\hat{m}_i\\right)^2\\right] + \\frac{2}{n}\\sigma^2(p + 1)\n",
    "$$\n",
    "\n",
    "where $p$ is the number of model predictors (size of $\\beta$ plus 1 for the intercept constant) and $\\sigma$ is the standard deviation of the noise (and is unknown). The first term on the right can be estimated using the MSE. The second term is the **optimism** of the model, which:\n",
    "\n",
    "- Grows with increasing variance - more noise gives more chances to overfit.\n",
    "- Shrinks with data size $n$.\n",
    "- Grows with the complexity of the model.\n",
    "\n",
    "This sketch shows why the out-of-sample MSE is guaranteed to be larger than the in-sample one. An unbiased estimator for out-of-sample MSE requires we also estimate the optimism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mallow's $C_p$ Statistic\n",
    "\n",
    "Let's say we're trying to use the above formulation to compare the efficacy of several linear models. A straightforward estimate of $\\sigma$ is the $\\hat{\\sigma}$ estimated using the most complex model that we wish to consider. *I assume we'll calculate that from the sample variance of the residuals between the data and model.* We also replace the expected in-sample MSE with the in-sample MSE itself (the only estimate we have):\n",
    "\n",
    "$$\n",
    "C_p = \\frac{1}{n}\\sum_{i=1}^n(Y_i - \\hat{m})^2 + \\frac{2}{n}\\hat{\\sigma}^2(p + 1)\n",
    "$$\n",
    "\n",
    "and the change in $C_p$ when comparing models is:\n",
    "\n",
    "$$\n",
    "\\Delta C_p = \\mathrm{MSE}_1 - \\mathrm{MSE}_2 + \\frac{2}{n}\\hat{\\sigma}^2(p_1 - p_2)\n",
    "$$\n",
    "\n",
    "An alternate definition of $C_p$\n",
    "\n",
    "$$\n",
    "C_p = \\frac{n}{\\hat{\\sigma}^2}\\mathrm{MSE} - n + 2p\n",
    "$$\n",
    "\n",
    "but for model comparison purposes this results in a $\\Delta C_p$ proportional to the one defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ and Adjusted $R^2$\n",
    "\n",
    "Recall that\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\mathrm{MSE}}{s_Y^2}\n",
    "$$\n",
    "\n",
    "where $s_Y^2$ is the sample variance. Maximizing this is equivalent to minimizing the in-sample MSE (since $s_Y^2$ doesn't change), so is not optimizing for out-of-sample MSE.\n",
    "\n",
    "The adjustice $R^2$ is\n",
    "\n",
    "$$\n",
    "R_\\mathrm{adj}^2 = 1 - \\frac{n}{n - p - 1}\\frac{\\mathrm{MSE}}{s_Y^2}\n",
    "$$\n",
    "\n",
    "Doing some Taylor expanding (Eqns. 10 - 12 Shalizi), we can approximate this as\n",
    "\n",
    "$$\n",
    "R_\\mathrm{adj}^2 \\approx 1 - \\frac{1}{s_Y^2}\\left(\\mathrm{MSE} + \\mathrm{MSE}\\frac{p + 1}{n}\\right)\n",
    "$$\n",
    "\n",
    "which should remind you of the optimism. Maximizing $R_\\mathrm{adj}^2$ is therefore somewhat similar to minimizing the out-of-sample MSE, but with an underweighted complexity penalty so it still doesn't lead to a true out-of-sample MSE minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC\n",
    "\n",
    "The Akaike Information Criterion (AIC) is given by\n",
    "\n",
    "$$\n",
    "\\mathrm{AIC}(S) = L_S - \\mathrm{dim}(S)\n",
    "$$\n",
    "\n",
    "where $L_S$ is the log-likelihood of the model $S$ and $\\mathrm{dim}(S)$ the number of free parameters. Hirotugu Akaike showed $AIC / n$ is an unbiased estimate of the expected log-probability the estimated parameters will give to a new data point, if the model is correct.\n",
    "\n",
    "For OLS, recall the likelihood resembles:\n",
    "\n",
    "$$\n",
    "L = -\\frac{n}{2}\\left(1 + 2\\pi\\right) - \\frac{n}{2}\\ln(\\mathrm{MSE})\n",
    "$$\n",
    "\n",
    "and the dimensions of the model are\n",
    "\n",
    "$$\n",
    "p + 2\n",
    "$$\n",
    "\n",
    "($+2$ for the constant and $\\sigma^2$ in the error). The difference between the AIC of two models is thus\n",
    "\n",
    "$$\n",
    "\\Delta \\mathrm{AIC} = -\\frac{n}{2}\\log\\left(1 + \\frac{\\Delta\\mathrm{MSE}}{\\mathrm{MSE}_1}\\right) - (p_1 - p_2).\n",
    "$$\n",
    "\n",
    "Assume that both models approximate the truth, so $\\mathrm{MSE}_1 \\sim \\hat{\\sigma}^2$, and we're at the limit where $\\Delta\\mathrm{MSE} / \\mathrm{MSE}_1 \\ll 1$; then:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathrm{AIC} \\approx -\\frac{n}{2}\\frac{\\Delta\\mathrm{MSE}}{\\hat{\\sigma}^2}- (p_1 - p_2),\n",
    "$$\n",
    "\n",
    "\n",
    "so the above is approximately proportional to the difference between their Mallow's $C_p$ (though this isn't the case for any model), so we see that maximizing the AIC is equivalent to minimizing the out-of-sample MSE.\n",
    "\n",
    "`statsmodels` fits include the AIC under `results.aic` (and the log-likelihood under `results.llf`, if you want to calculate AIC yourself).\n",
    "\n",
    "#### Addendum\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion#How_to_use_AIC_in_practice) (and [this paper](https://link.springer.com/content/pdf/10.3758/BF03206482.pdf), Eqn. 3), the **relative likelihood** of one model to the best model is given by\n",
    "\n",
    "$$\n",
    "L(M_i|{\\bf x}) \\propto \\exp\\left(-\\frac{1}{2}(AIC_i - AIC_\\mathrm{min})\\right)\n",
    "$$\n",
    "\n",
    "because \"AIC is an unbiased estimator of minus twice the expected log likelihood of the model\" (from the paper, just above Eqn. 3). Wikipedia suggests this as method for model selection - if $L$ is within an order of magnitude of 1, the selection algorithm cannot rule it out. This is obviously a rule of thumb (the relative likelihood is related to but not exactly equal to the likelihood ratio), but we can adopt it for selecting models that may have slightly more or slightly fewer independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out Cross Validation\n",
    "\n",
    "Leave-one-out cross validation (LOOCV) fits to all data save one point, denoted with superscript $(-i)$ to indicate that $i$ was left out of the fit. The fitted results are denoted $\\hat{m}_i^{(-i)}$. The LOOCV is defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{LOOCV} = \\frac{1}{n}\\sum_{i=1}^n(Y_i - \\hat{m}_i^{(-i)})^2\n",
    "$$\n",
    "\n",
    "This can be rapidly estimated by fitting with all data and weighting each square error term by $1 - H_{ii}$, where $H$ is the [leverage](https://en.wikipedia.org/wiki/Leverage_(statistics)). At the limit of large $n$, LOOCV converges to Mallow's $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing LOOCV, AIC and $C_P$\n",
    "\n",
    "While Shalizi's course is about linear models, there are theorems for a broad range of cirumstances that all say something like:\n",
    "\n",
    "> As $n\\rightarrow \\infty$ the expected out of sample MSE of the model picked by LOOCV is close to the best model considered.\n",
    "\n",
    "These theorems don't require Gaussian noise or linearity of the model.\n",
    "\n",
    "For large $n$, $C_p$ and AIC will pick out the same model as LOOCV, but $C_p$ and AIC are faster to calculate. The best way to think about $C_p$ and AIC is they're fast ways of approximating LOOCV.\n",
    "\n",
    "On the other hand, as $n\\rightarrow\\infty$ we also see that all three methods pick out models that are bigger than the true model because the methods all give *unbiased* estimates of the generalization error, but place no restrictions on the *variance* of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Fold Cross-Validation\n",
    "\n",
    "Generalization of LOOCV that expands the number of points in the holdout dataset to $k$. Procedure:\n",
    "\n",
    "- Divide dataset up into $k$-subsets, or \"folds\".\n",
    "- For each fold, perform a best fit of the model on all other folds of data, then calculate the MSE of the current fold.\n",
    "- Average MSEs together.\n",
    "\n",
    "The main advantage is that for each data point in LOOCV the data being fit to is very similar compared to every other data point (since only one point is being removed), meaning LOOCV does very little to reduce the variance. Removing more data (typical $k \\approx 5-10$) leads to more control for variance (because a deviant fold will more greatly affect the average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC\n",
    "\n",
    "The Bayesian Information Criterion (BIC) is given by\n",
    "\n",
    "$$\n",
    "BIC(S) = L_S - \\frac{\\ln(n)}{2}\\mathrm{dim}(S)\n",
    "$$\n",
    "\n",
    "This is a stronger penalty than AIC, and so as $n\\rightarrow\\infty$ if the true model is among those BIC can select, BIC will tend to pick it. In practice it tends to work less well than AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Model Selection\n",
    "\n",
    "One way to auto-select a model is to begin with the largest model possible, then selectively eliminate coefficients until an optimal model is achieved. The criteria can be:\n",
    "\n",
    "- The least significant coefficient. Stop when all coefficients are significant.\n",
    "- The coefficient that best improves whatever score we're trying to minimize. Stop when the optimal model is achieved.\n",
    "\n",
    "Stepwise selection can be done forwards or backwards (or forward-backward).\n",
    "\n",
    "The algorithm is greedy (even forward-backward), and therefore cannot guarantee consistency. Indeed, Shalizi takes a dim view to its efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('bditto': venv)",
   "language": "python",
   "name": "python36964bitbdittovenvd56842b7ca0c4929925d70f8c4c7f13a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
